	• PCIe 기반 NVMe SSD, 리눅스 커널
	=> I/O요청 병렬처리를 위한 멀티 큐 지원
	• 멀티 큐 블록 계층 
	- 소프트웨어 스케이징 큐
	-> I/O 요청 스케줄링->하드웨어 큐로 전송
	**멀티 큐 I/O 스케줄러: None, Kyber, BFQ, MQ-deadline 등
	None:  I/O요청 병합만 수행 (정렬X) -> 최소 오버헤드
	Kyber: 동기 요청 큐, 비동기 요청 큐 / 읽기ㅗ, 동기적 쓰기 요청에 대해 목표 지연 시간 설정 -> 자체적으로 큐 깊이 조정 
	BFQ: 각 프로세스가 처리해야 할 I/O 요청의 섹터 수를 기반으로 타임슬라이스 처럼 budget할당 -> budget만큼 동작 시간 보장 
	MQ-deadline: 단일 큐 스케줄러의 deadline I/O 스케줄러를 멀티 큐 I/O환에서 사용 / 정렬 큐, 읽기 FIFO 큐, 쓰기 FIFO 큐 / FIFO 특정 deadline 有
	- 하드웨어 디스패치 큐 
	->FIFO방식으로 I/O요청을 NVMe 디바이스 드라이버에 전송 -> I/O 요청 처리 
	• I/O요청을 한 프로세스가 I/O완료 확인하는 방식
	- 인터럽트 방식:
	context switching을 통해 I/O 요청을 한 프로세스를 I/O 완료 인터럽트가 올때까지 해당 프로세스를 휴면상태로 전환
	-> CPU 효과적 사용 / context switching으로 인한 오버헤드
	- 폴링 방식:
	context switching을 하지 않고, 지속적으로 I/O가 완료되었는지 직접 확인
	-> CPU 비효율적 사용 / 빠른 I/O처리가 가능한 고성능저장치에서는 효과적
	• None에서만 폴링 방식이 제대로 동작함.  
