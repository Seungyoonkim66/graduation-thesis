	v NVMe : Non-Volatile Memory Express (비휘발성 기억장치 익스프레스)
	= 비휘발성 메모리 호스트 컨트롤러 인터페이스 사양
	**NVM : Non-Volatile Memory (비휘발성 메모리)
	= 주로 SSD 형태로 출시되는 플래시 메모리를 가리킨다.  
	**PCI expree : 입출력을 위한 직렬 구조의 인터페이스 
	->최근 하드웨어 I/O 가상화 지원한다. 높은 시스템 버스 대역폭, 적은 I/O 핀 수, 적은 물리적 면적, 버스 장치에 대한 성능 확장성, 상세한 오류 검출 및 보고 구조
	• 현대의 SSD에서 볼 수 있는 여러 수준의 병렬화 허용 
	-> 호스트 하드웨어와 소프트웨어에 의해 완전히 이용될 수 있음
	-> I/O 오버헤드 줄임
	+ 길이가 긴 multi instruction queue, reduced latency 
	=> 성능 개성된 논리 장치 인터페이스 
	• PCIe SSD의 낮은 latency와 parallelism + 동시대 cpu(?), app parallelism
	=> 호스트 하드웨어와 소프트웨어의 paralleism 이용 가능 
	** 다중 명령어 큐의 길이가 길어서 큐 하나 당 최대 명령어의 개수가 AHCI에 비해 매우 많다. 병렬화를 이용하여 다중 스레드 환경에서도 락이 없다. / 
	높은 확장성과 성능 
	Ø 소프트웨어 큐와 하드웨어 큐의 비율을 조절하여 I/O 성능 분석 
	결론: 소프트웨어 큐: 하드웨어 큐 = 8 : 1 성능 좋게 나옴. 
	
	
	• SSD의 등장과 발전으로 저장장치로 인한 시스템 I/O 성능 제약 줄어들고 있음 
	-> 소프트웨어 오버헤드의 영향 증가 
	-> 리눅스 커널 I/O 스택 재구성 및  최적화 필요 :
		그 중에 하나가 멀티 큐 블롱 계층 연구 
		(기존 단일 블록 계층은 다수의 프로세스가 동시에 큐에 접근하려는 경우 과도한 락 경쟁이 발생하여 I/O 성능 오버헤드가 크다. 이를 보완하기 위해 멀티 큐 블록 계층 제안되었다.)
	v 멀티 큐 블록 계층 :
	- CPU 코어 수 만큼 큐 생성 -> 1코어 당 1 큐 
	- 소프트웨어 큐: I/O 요청 병합 및 스케줄링
	=> cpu 코어 수만큼 생성됨 -> 락 경쟁 완전 해소 
	- 하드웨어 큐 : I/O 요청들을 디바이스 드라이버로 전달하여 병합 및 스케줄링
	=> 저장장치자 지원할 수 있는 만큼만 생성 가능
	- 소프트웨어 큐와 하드웨어 큐 라운드로빈 방식으로 연결 
	• NVMe: 디바이스 드라이버 계층에서 다수의 큐를 지원하는 인터페이스 
	<제출큐, 완료큐> 여러 개 쌍 생성 
	저장장치에서 제공하는 만큼만 생성 가능 
	- 최대 65,536개의 하드웨어 큐 지원 가능 (실제 저장장치의 제약 有)
	- submission queue 제출 큐: NVMe명령을 전달 받고, 저장장치에 전달
	- completion queue 완료 큐: 저장장치에서 I/O 요청이 완료 여부 확인하고 상위 계층에 완료 신호 전달 
	=> 다수의 하드웨어 큐에서 I/O 요청이 드라이버로 전달되어도 I/O 확장 가능하여 처리 성능이 향상
	- SQ CQ 도 라운드 로빈 방식으로 코어와 연결 
	- NVMe 저장장치에 I/O요청 발생 -> 커널이 해당 I/O 요청에 대한 명령을 SQ에 표시 -> doorbell 레지스터를 통해 저장장치에 요청 신호 전달 -> 저장장치 I/O 요청 완료 -> 완료 결과를 CQ에 표시하고 호스트 시스템에 인터럽트 발생시킴 -> I/O 요청 완료 처리  
